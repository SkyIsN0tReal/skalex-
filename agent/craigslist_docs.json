{
  "pages": [
    {
      "title": "Quickstart",
      "content": [
        {
          "type": "text",
          "value": "Welcome to the documentation for the Craigslist Web API. This guide will walk you through the process of authenticating and making requests to search for postings. The API is primarily used internally by the Craigslist website to dynamically load search results.\n\nThe process involves three main steps:\n1. Initializing a session to obtain the correct regional subdomain and necessary cookies.\n2. Performing a search against the Search API (`sapi.craigslist.org`).\n3. Parsing the complex array-based response structure to extract meaningful data.\n\nBelow is a quick example of performing a search for apartments in the SF Bay Area."
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "import requests\n\n# Use a session to persist cookies across requests\nsession = requests.Session()\nsession.headers.update({\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'\n})\n\n# 1. Initialize session and get region-specific cookies\nsession.get('https://sfbay.craigslist.org/')\n\n# 2. Define search parameters for SF Bay Area apartments ('apa')\nsearch_params = {\n    'batch': '1-0-360-0-0',\n    'searchPath': 'apa',\n    'lang': 'en',\n    'cc': 'us'\n}\n\n# 3. Make the search request\nresponse = session.get('https://sapi.craigslist.org/web/v8/postings/search/full', params=search_params)\n\nif response.status_code == 200:\n    data = response.json()\n    items = data['data']['items']\n    location_descriptions = data['data']['decode']['locationDescriptions']\n    \n    print(f\"Found {len(items)} results\")\n    \n    # Parse first result (items are arrays, not objects!)\n    if items:\n        first_item = items[0]\n        title = first_item[-1]  # Title is always the last element\n        \n        # Extract price from nested array structure\n        price = 'No price'\n        if len(first_item) > 8 and isinstance(first_item[8], list) and len(first_item[8]) > 1:\n            price = first_item[8][1]\n        \n        # Decode location using location descriptions\n        location = 'Unknown'\n        if len(first_item) > 2 and first_item[2] < len(location_descriptions):\n            location = location_descriptions[first_item[2]]\n        \n        print(f\"First result: {title} - {price} - {location}\")\nelse:\n    print(f\"Failed to fetch data: {response.status_code}\")",
            "cURL": "# Note: You must first get cookies by visiting the regional site\ncurl -c cookies.txt 'https://sfbay.craigslist.org/'\n\n# Then use the cookies for the API request\ncurl -b cookies.txt 'https://sapi.craigslist.org/web/v8/postings/search/full?batch=1-0-360-0-0&searchPath=apa&lang=en&cc=us' \\\n  -H 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'"
          }
        }
      ]
    },
    {
      "title": "Authentication",
      "content": [
        {
          "type": "text",
          "value": "The Craigslist API uses a cookie-based session management system. To successfully make API requests, you must first visit the Craigslist website to establish a session and obtain the necessary cookies. There is no explicit login required for searching.\n\nThe process is as follows:\n\n1.  **Initial Visit & Geolocation:** A request to `https://www.craigslist.org/` will result in a `302 Found` redirect. This first response sets the primary `cl_b` cookie.\n\n2.  **Regional Redirect:** The user is redirected to `https://geo.craigslist.org/`, which determines the user's geographical region based on their IP address. This, in turn, redirects to the appropriate regional subdomain (e.g., `https://sfbay.craigslist.org`).\n\n3.  **Regional Cookie:** Upon landing on the regional subdomain, a `cl_def_hp` cookie is set (e.g., `cl_def_hp=sfbay`). This cookie indicates the user's default region.\n\nBoth the `cl_b` and `cl_def_hp` cookies must be included in all subsequent requests to the Search API (`sapi.craigslist.org`). You can also visit a regional subdomain directly (e.g., `https://sfbay.craigslist.org/`) to get the necessary cookies more efficiently."
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "import requests\n\n# Using a session object handles cookies automatically\nsession = requests.Session()\nsession.headers.update({\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'\n})\n\n# Option 1: Visit regional subdomain directly (recommended)\nresponse = session.get('https://sfbay.craigslist.org/')\nprint(f\"Direct regional visit successful: {response.status_code == 200}\")\n\n# Option 2: Let redirects determine your region\n# response = session.get('https://www.craigslist.org/', allow_redirects=True)\n# print(f\"Final URL after redirects: {response.url}\")\n\n# Check acquired cookies\nprint(\"Cookies acquired:\")\nfor name, value in session.cookies.items():\n    print(f\"- {name}: {value}\")\n\n# The session is now ready for API calls"
          }
        }
      ]
    },
    {
      "title": "Response Structure",
      "content": [
        {
          "type": "text",
          "value": "Understanding the response structure is crucial for successfully parsing Craigslist API data. The API returns complex nested arrays rather than simple key-value objects, which requires careful handling."
        },
        {
          "type": "text",
          "value": "### Basic Response Format\n\nAll search endpoints return a JSON object with this structure:"
        },
        {
          "type": "code_snippet",
          "languages": {
            "json": "{\n  \"apiVersion\": 8,\n  \"data\": {\n    \"items\": [...],  // Array of posting data (each item is an array)\n    \"decode\": {\n      \"locationDescriptions\": [...] // Array for decoding location indices\n    },\n    \"cacheId\": \"...\",  // Required for pagination\n    \"cacheTs\": 1755962904,\n    \"canonicalUrl\": \"//sfbay.craigslist.org/search/apa\",\n    \"categoryAbbr\": \"apa\"\n  }\n}"
          }
        },
        {
          "type": "text",
          "value": "### Item Array Structure\n\n**Critical**: Each item in the `items` array is itself an array (not an object) with data at specific indices:"
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "# Each item follows this array structure:\n# [\n#   post_id,              # item[0] - Unique posting ID\n#   internal_id,          # item[1] - Internal Craigslist ID  \n#   location_index,       # item[2] - Index into locationDescriptions array\n#   numeric_price,        # item[3] - Price as number\n#   coordinates_string,   # item[4] - Geographic coordinates\n#   some_identifier,      # item[5] - Unknown identifier\n#   [4, image_ids...],    # item[6] - Array starting with 4, followed by image IDs\n#   [6, url_slug],        # item[7] - Array with 6 and URL slug\n#   [10, formatted_price], # item[8] - Array with 10 and formatted price string\n#   \"Posting Title\"       # item[9 or -1] - Title text (always last element)\n# ]\n\n# Example item:\n[\n  7775610,\n  99238, \n  7,\n  1700,\n  \"1:1:1~37.7749~-122.4194\",\n  \"abc123\",\n  [4, \"img1\", \"img2\"],\n  [6, \"apartment-downtown-1br\"],\n  [10, \"$1,700\"],\n  \"Beautiful 1BR Apartment Downtown\"\n]"
          }
        },
        {
          "type": "text",
          "value": "### Safe Parsing Function\n\nHere's a robust function to parse item arrays:"
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "def parse_craigslist_item(item, location_descriptions):\n    \"\"\"\n    Parse a Craigslist item array into a readable dictionary.\n    \n    Args:\n        item: The item array from API response\n        location_descriptions: Array from response['data']['decode']['locationDescriptions']\n    \n    Returns:\n        Dictionary with parsed data\n    \"\"\"\n    try:\n        # Title is always the last element\n        title = item[-1] if item else 'No title'\n        \n        # Extract price from formatted array or fallback to numeric\n        price = 'No price'\n        if len(item) > 8 and isinstance(item[8], list) and len(item[8]) > 1:\n            price = item[8][1]  # Formatted price like \"$1,700\"\n        elif len(item) > 3 and isinstance(item[3], (int, float)):\n            price = f\"${item[3]}\"  # Fallback to numeric price\n        \n        # Decode location using location descriptions array\n        location = 'Location not specified'\n        if (len(item) > 2 and isinstance(item[2], int) and \n            0 <= item[2] < len(location_descriptions)):\n            location = location_descriptions[item[2]]\n        \n        # Extract URL slug from nested array\n        url_slug = ''\n        if (len(item) > 7 and isinstance(item[7], list) and \n            len(item[7]) > 1 and item[7][1]):\n            url_slug = item[7][1]\n        \n        # Extract post ID\n        post_id = item[0] if len(item) > 0 else None\n        \n        return {\n            'post_id': post_id,\n            'title': title,\n            'price': price,\n            'location': location,\n            'url_slug': url_slug,\n            'coordinates': item[4] if len(item) > 4 else None\n        }\n        \n    except (IndexError, TypeError, AttributeError) as e:\n        return {\n            'post_id': None,\n            'title': 'Parse error',\n            'price': 'Unknown',\n            'location': 'Unknown',\n            'url_slug': '',\n            'error': str(e)\n        }"
          }
        },
        {
          "type": "text",
          "value": "### Location Decoding\n\nLocation information requires decoding using the `locationDescriptions` array:"
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "# Example locationDescriptions array:\n# [0, \"san jose north\", \"alameda\", \"potrero hill\", \"San Leandro\", ...]\n\ndef decode_location(location_index, location_descriptions):\n    \"\"\"\n    Decode a location index to a human-readable location name.\n    \n    Args:\n        location_index: Integer index from item[2]\n        location_descriptions: Array from response decode section\n    \n    Returns:\n        String location name\n    \"\"\"\n    if (isinstance(location_index, int) and \n        0 <= location_index < len(location_descriptions)):\n        return location_descriptions[location_index]\n    elif location_index == 0:\n        return 'General area'  # Index 0 often means general region\n    else:\n        return 'Location not specified'\n\n# Usage:\nlocation_descriptions = response_data['data']['decode']['locationDescriptions']\nfor item in items:\n    location = decode_location(item[2], location_descriptions)\n    print(f\"Location: {location}\")"
          }
        },
        {
          "type": "text",
          "value": "### Complete Parsing Example\n\nHere's how to parse an entire search response:"
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "def parse_search_response(response_data):\n    \"\"\"\n    Parse a complete Craigslist search response.\n    \n    Args:\n        response_data: JSON response from the API\n    \n    Returns:\n        List of parsed items\n    \"\"\"\n    if not response_data or 'data' not in response_data:\n        return []\n    \n    data = response_data['data']\n    items = data.get('items', [])\n    location_descriptions = data.get('decode', {}).get('locationDescriptions', [])\n    \n    parsed_items = []\n    for item in items:\n        try:\n            parsed_item = parse_craigslist_item(item, location_descriptions)\n            parsed_items.append(parsed_item)\n        except Exception as e:\n            print(f\"Error parsing item: {e}\")\n            continue\n    \n    return parsed_items\n\n# Usage:\nresponse = session.get('https://sapi.craigslist.org/web/v8/postings/search/full', params=search_params)\nif response.status_code == 200:\n    data = response.json()\n    parsed_results = parse_search_response(data)\n    \n    for result in parsed_results[:5]:  # Show first 5\n        if 'error' not in result:\n            print(f\"{result['title']} - {result['price']} - {result['location']}\")\n        else:\n            print(f\"Parse error: {result['error']}\")"
          }
        }
      ]
    },
    {
      "title": "Endpoints",
      "content": [
        {
          "type": "text",
          "value": "### Initial Search\nThis endpoint retrieves the first set of search results and essential metadata for pagination.\n\n**URL:** `https://sapi.craigslist.org/web/v8/postings/search/full`\n\n**Method:** `GET`\n\n**Query Parameters:**\n- `batch`: A string that defines the initial fetch. For the first request, this is typically `1-0-360-0-0`.\n- `searchPath`: The category abbreviation (e.g., `apa` for apartments for rent, `sss` for for-sale).\n- `query` (optional): The user-entered search term (e.g., `downtown`).\n- `lang`: The language code (e.g., `en`).\n- `cc`: The country code (e.g., `us`).\n\nThe response contains a `cacheId` and `cacheTs` which are crucial for fetching subsequent pages of results."
        },
        {
          "type": "text",
          "value": "### Paginated Search (Batch)\nThis endpoint is used to fetch subsequent pages of search results (postings) in batches.\n\n**URL:** `https://sapi.craigslist.org/web/v8/postings/search/batch`\n\n**Method:** `GET`\n\n**Query Parameters:**\n- `batch`: A multi-part string for pagination. Example: `1-1080-1080-1-1-1755964085-1755964104`. The key parts are the second and third numbers, which represent `offset` and `count`, respectively. To get the next page, increment the offset (e.g., `1-2160-1080-...`).\n- `cacheId`: The unique identifier for the search result set, obtained from the initial `/search/full` response.\n- `lang`: The language code.\n- `cc`: The country code."
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "# Example pagination implementation\ndef get_next_page(session, cache_id, cache_ts, page_offset=1080):\n    \"\"\"Get the next page of search results.\"\"\"\n    # Calculate timestamps for batch parameter\n    start_ts = cache_ts - 30\n    end_ts = cache_ts\n    \n    batch_str = f\"1-{page_offset}-1080-6-0-{start_ts}-{end_ts}\"\n    \n    params = {\n        'batch': batch_str,\n        'cacheId': cache_id,\n        'lang': 'en',\n        'cc': 'us'\n    }\n    \n    response = session.get(\n        'https://sapi.craigslist.org/web/v8/postings/search/batch',\n        params=params\n    )\n    \n    if response.status_code == 200:\n        data = response.json()\n        # Paginated results are in data['data']['batch']\n        return data['data'].get('batch', [])\n    return []"
          }
        },
        {
          "type": "text",
          "value": "### Search Suggestions\nProvides search term suggestions as a user types into the search bar.\n\n**URL:** `https://{region}.craigslist.org/suggest`\n\n**Method:** `GET`\n\n**Query Parameters:**\n- `type`: Must be set to `search`.\n- `cat`: The category abbreviation (e.g., `apa`).\n- `term`: The partial search term being typed (e.g., `downto`)."
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "# Get search suggestions\ndef get_suggestions(session, region, category, partial_term):\n    \"\"\"Get search suggestions for a partial term.\"\"\"\n    params = {\n        'type': 'search',\n        'cat': category,\n        'term': partial_term\n    }\n    \n    response = session.get(\n        f'https://{region}.craigslist.org/suggest',\n        params=params\n    )\n    \n    if response.status_code == 200:\n        return response.json()  # Returns array of suggestions\n    return []\n\n# Usage:\nsuggestions = get_suggestions(session, 'sfbay', 'apa', 'downto')\nprint(suggestions)  # ['downtown', 'downtown san jose', ...]"
          }
        }
      ]
    },
    {
      "title": "Python Examples",
      "content": [
        {
          "type": "text",
          "value": "### Example 1: Complete Search Implementation\nThis script demonstrates proper session initialization, searching, and parsing with the correct array structure."
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "import requests\n\ndef search_craigslist(region, category, query=None, max_results=10):\n    \"\"\"\n    Search Craigslist and return parsed results.\n    \n    Args:\n        region: Regional subdomain (e.g., 'sfbay')\n        category: Category code (e.g., 'apa' for apartments)\n        query: Optional search term\n        max_results: Maximum number of results to return\n    \n    Returns:\n        List of parsed posting dictionaries\n    \"\"\"\n    session = requests.Session()\n    session.headers.update({\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'\n    })\n\n    # Initialize session to get cookies\n    try:\n        init_response = session.get(f'https://{region}.craigslist.org/', timeout=10)\n        init_response.raise_for_status()\n        print(f\"Session initialized for {region}\")\n    except requests.RequestException as e:\n        print(f\"Error initializing session: {e}\")\n        return []\n\n    # Prepare search parameters\n    search_params = {\n        'batch': '1-0-360-0-0',\n        'searchPath': category,\n        'lang': 'en',\n        'cc': 'us'\n    }\n    \n    if query:\n        search_params['query'] = query\n\n    # Perform search\n    try:\n        response = session.get(\n            'https://sapi.craigslist.org/web/v8/postings/search/full',\n            params=search_params,\n            timeout=10\n        )\n        response.raise_for_status()\n    except requests.RequestException as e:\n        print(f\"Error during search: {e}\")\n        return []\n\n    # Parse response\n    data = response.json()\n    if 'data' not in data or 'items' not in data['data']:\n        print(\"Unexpected response format\")\n        return []\n\n    items = data['data']['items']\n    location_descriptions = data['data']['decode'].get('locationDescriptions', [])\n    \n    print(f\"Found {len(items)} results\")\n    \n    # Parse items using correct array structure\n    parsed_results = []\n    for item in items[:max_results]:\n        try:\n            # Extract data from array positions\n            post_id = item[0] if len(item) > 0 else None\n            title = item[-1] if item else 'No title'\n            \n            # Extract price from nested array structure\n            price = 'No price'\n            if len(item) > 8 and isinstance(item[8], list) and len(item[8]) > 1:\n                price = item[8][1]\n            elif len(item) > 3 and isinstance(item[3], (int, float)):\n                price = f\"${item[3]}\"\n            \n            # Decode location\n            location = 'Unknown location'\n            if (len(item) > 2 and isinstance(item[2], int) and\n                0 <= item[2] < len(location_descriptions)):\n                location = location_descriptions[item[2]]\n            \n            # Extract URL slug\n            url_slug = ''\n            if (len(item) > 7 and isinstance(item[7], list) and\n                len(item[7]) > 1 and item[7][1]):\n                url_slug = item[7][1]\n                full_url = f\"https://{region}.craigslist.org/{url_slug}\"\n            else:\n                full_url = ''\n            \n            parsed_results.append({\n                'post_id': post_id,\n                'title': title,\n                'price': price,\n                'location': location,\n                'url': full_url\n            })\n            \n        except (IndexError, TypeError) as e:\n            print(f\"Error parsing item: {e}\")\n            continue\n    \n    return parsed_results\n\nif __name__ == '__main__':\n    # Search for apartments in SF Bay Area\n    results = search_craigslist('sfbay', 'apa', 'downtown', max_results=5)\n    \n    if results:\n        print(\"\\n--- Search Results ---\")\n        for i, result in enumerate(results, 1):\n            print(f\"{i}. {result['title']}\")\n            print(f\"   Price: {result['price']}\")\n            print(f\"   Location: {result['location']}\")\n            if result['url']:\n                print(f\"   URL: {result['url']}\")\n            print()\n    else:\n        print(\"No results found or error occurred\")"
          }
        },
        {
          "type": "text",
          "value": "### Example 2: Pagination Implementation\nThis script shows how to correctly paginate through search results using the cacheId."
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "import requests\nimport time\n\ndef paginated_search(region, category, query=None, total_pages=3):\n    \"\"\"\n    Search Craigslist and paginate through multiple pages of results.\n    \n    Args:\n        region: Regional subdomain\n        category: Category code\n        query: Optional search term\n        total_pages: Number of pages to fetch\n    \n    Returns:\n        List of all parsed results from all pages\n    \"\"\"\n    session = requests.Session()\n    session.headers.update({\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'\n    })\n\n    # Initialize session\n    session.get(f'https://{region}.craigslist.org/')\n    \n    # Initial search\n    search_params = {\n        'batch': '1-0-360-0-0',\n        'searchPath': category,\n        'lang': 'en',\n        'cc': 'us'\n    }\n    \n    if query:\n        search_params['query'] = query\n    \n    print(\"Fetching initial results...\")\n    response = session.get(\n        'https://sapi.craigslist.org/web/v8/postings/search/full',\n        params=search_params\n    )\n    \n    if response.status_code != 200:\n        print(f\"Initial search failed: {response.status_code}\")\n        return []\n    \n    initial_data = response.json()\n    cache_id = initial_data['data'].get('cacheId')\n    cache_ts = initial_data['data'].get('cacheTs')\n    location_descriptions = initial_data['data']['decode'].get('locationDescriptions', [])\n    \n    if not cache_id:\n        print(\"Could not get cacheId for pagination\")\n        return []\n    \n    print(f\"Got cacheId for pagination: {cache_id[:30]}...\")\n    \n    # Parse initial results\n    all_results = []\n    initial_items = initial_data['data']['items']\n    \n    def parse_items(items, location_descriptions):\n        \"\"\"Helper function to parse item arrays.\"\"\"\n        parsed = []\n        for item in items:\n            try:\n                title = item[-1] if item else 'No title'\n                price = 'No price'\n                if len(item) > 8 and isinstance(item[8], list) and len(item[8]) > 1:\n                    price = item[8][1]\n                elif len(item) > 3:\n                    price = f\"${item[3]}\"\n                \n                location = 'Unknown'\n                if (len(item) > 2 and isinstance(item[2], int) and\n                    0 <= item[2] < len(location_descriptions)):\n                    location = location_descriptions[item[2]]\n                \n                parsed.append({\n                    'title': title,\n                    'price': price,\n                    'location': location,\n                    'page': 1\n                })\n            except (IndexError, TypeError):\n                continue\n        return parsed\n    \n    all_results.extend(parse_items(initial_items, location_descriptions))\n    print(f\"Page 1: {len(initial_items)} results\")\n    \n    # Fetch additional pages\n    for page in range(2, total_pages + 1):\n        offset = (page - 1) * 1080\n        start_ts = cache_ts - 30\n        end_ts = cache_ts\n        \n        batch_str = f\"1-{offset}-1080-6-0-{start_ts}-{end_ts}\"\n        \n        pagination_params = {\n            'batch': batch_str,\n            'cacheId': cache_id,\n            'lang': 'en',\n            'cc': 'us'\n        }\n        \n        print(f\"Fetching page {page}...\")\n        \n        try:\n            page_response = session.get(\n                'https://sapi.craigslist.org/web/v8/postings/search/batch',\n                params=pagination_params\n            )\n            \n            if page_response.status_code == 200:\n                page_data = page_response.json()\n                page_items = page_data['data'].get('batch', [])\n                \n                if page_items:\n                    parsed_page = parse_items(page_items, location_descriptions)\n                    for item in parsed_page:\n                        item['page'] = page\n                    all_results.extend(parsed_page)\n                    print(f\"Page {page}: {len(page_items)} results\")\n                else:\n                    print(f\"Page {page}: No more results\")\n                    break\n            else:\n                print(f\"Page {page} failed: {page_response.status_code}\")\n                break\n                \n        except requests.RequestException as e:\n            print(f\"Error fetching page {page}: {e}\")\n            break\n        \n        # Be respectful - add delay between requests\n        time.sleep(1)\n    \n    return all_results\n\nif __name__ == '__main__':\n    # Search for bicycles across multiple pages\n    all_results = paginated_search('sfbay', 'sss', 'bicycle', total_pages=3)\n    \n    print(f\"\\nTotal results across all pages: {len(all_results)}\")\n    \n    # Show results by page\n    for page in range(1, 4):\n        page_results = [r for r in all_results if r['page'] == page]\n        if page_results:\n            print(f\"\\n--- Page {page} Sample Results ---\")\n            for result in page_results[:3]:  # Show first 3 from each page\n                print(f\"{result['title']} - {result['price']} - {result['location']}\")"
          }
        },
        {
          "type": "text",
          "value": "### Example 3: Error Handling and Robust Parsing\nThis example shows comprehensive error handling for the complex parsing requirements."
        },
        {
          "type": "code_snippet",
          "languages": {
            "python": "import requests\nimport json\nfrom typing import List, Dict, Optional\n\ndef robust_craigslist_search(region: str, category: str, query: Optional[str] = None) -> List[Dict]:\n    \"\"\"\n    Robust Craigslist search with comprehensive error handling.\n    \n    Args:\n        region: Regional subdomain (e.g., 'sfbay')\n        category: Category code (e.g., 'apa')\n        query: Optional search term\n    \n    Returns:\n        List of successfully parsed results\n    \"\"\"\n    session = requests.Session()\n    session.headers.update({\n        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36'\n    })\n\n    # Step 1: Initialize session with error handling\n    try:\n        init_response = session.get(f'https://{region}.craigslist.org/', timeout=10)\n        init_response.raise_for_status()\n        print(f\"‚úì Session initialized for {region}\")\n    except requests.Timeout:\n        print(\"‚úó Timeout during session initialization\")\n        return []\n    except requests.ConnectionError:\n        print(\"‚úó Connection error during session initialization\")\n        return []\n    except requests.RequestException as e:\n        print(f\"‚úó Error initializing session: {e}\")\n        return []\n\n    # Step 2: Prepare and validate search parameters\n    search_params = {\n        'batch': '1-0-360-0-0',\n        'searchPath': category,\n        'lang': 'en',\n        'cc': 'us'\n    }\n    \n    if query and query.strip():\n        search_params['query'] = query.strip()\n        print(f\"Searching for: '{query}' in category '{category}'\")\n    else:\n        print(f\"Browsing category '{category}' (no search term)\")\n\n    # Step 3: Perform search with error handling\n    try:\n        response = session.get(\n            'https://sapi.craigslist.org/web/v8/postings/search/full',\n            params=search_params,\n            timeout=15\n        )\n        response.raise_for_status()\n    except requests.Timeout:\n        print(\"‚úó Timeout during search request\")\n        return []\n    except requests.HTTPError as e:\n        print(f\"‚úó HTTP error during search: {e}\")\n        return []\n    except requests.RequestException as e:\n        print(f\"‚úó Error during search: {e}\")\n        return []\n\n    # Step 4: Parse JSON response with validation\n    try:\n        data = response.json()\n    except json.JSONDecodeError as e:\n        print(f\"‚úó Failed to parse JSON response: {e}\")\n        return []\n    \n    # Validate response structure\n    if 'data' not in data:\n        print(\"‚úó Response missing 'data' field\")\n        return []\n    \n    if 'items' not in data['data']:\n        print(\"‚úó Response missing 'items' field\")\n        return []\n    \n    items = data['data']['items']\n    location_descriptions = data['data'].get('decode', {}).get('locationDescriptions', [])\n    \n    if not location_descriptions:\n        print(\"‚ö† Warning: No location descriptions found\")\n    \n    print(f\"‚úì Found {len(items)} raw items to parse\")\n\n    # Step 5: Parse items with individual error handling\n    def safe_parse_item(item: List, index: int) -> Optional[Dict]:\n        \"\"\"\n        Safely parse a single item with detailed error handling.\n        \"\"\"\n        try:\n            # Validate item is a list with minimum length\n            if not isinstance(item, list) or len(item) < 3:\n                print(f\"‚ö† Item {index}: Invalid structure (not a list or too short)\")\n                return None\n            \n            # Extract title (always last element)\n            title = str(item[-1]) if item else 'No title'\n            if not title or title == 'No title':\n                print(f\"‚ö† Item {index}: Missing title\")\n                return None\n            \n            # Extract post ID\n            post_id = item[0] if len(item) > 0 and isinstance(item[0], (int, str)) else None\n            \n            # Extract price with multiple fallbacks\n            price = 'No price'\n            try:\n                # First try formatted price array\n                if (len(item) > 8 and isinstance(item[8], list) and \n                    len(item[8]) > 1 and item[8][1]):\n                    price = str(item[8][1])\n                # Fallback to numeric price\n                elif len(item) > 3 and isinstance(item[3], (int, float)):\n                    price = f\"${item[3]}\"\n            except (IndexError, TypeError, ValueError):\n                price = 'Price parse error'\n            \n            # Extract and decode location\n            location = 'Unknown location'\n            try:\n                if (len(item) > 2 and isinstance(item[2], int) and\n                    0 <= item[2] < len(location_descriptions)):\n                    location = str(location_descriptions[item[2]])\n                elif len(item) > 2 and item[2] == 0:\n                    location = 'General area'\n            except (IndexError, TypeError):\n                location = 'Location parse error'\n            \n            # Extract URL slug\n            url_slug = ''\n            try:\n                if (len(item) > 7 and isinstance(item[7], list) and\n                    len(item[7]) > 1 and item[7][1]):\n                    url_slug = str(item[7][1])\n            except (IndexError, TypeError):\n                pass  # URL is optional\n            \n            return {\n                'post_id': post_id,\n                'title': title,\n                'price': price,\n                'location': location,\n                'url_slug': url_slug,\n                'url': f\"https://{region}.craigslist.org/{url_slug}\" if url_slug else '',\n                'raw_item_length': len(item)\n            }\n            \n        except Exception as e:\n            print(f\"‚úó Item {index}: Unexpected parsing error: {e}\")\n            return None\n    \n    # Parse all items\n    successful_results = []\n    parse_errors = 0\n    \n    for i, item in enumerate(items):\n        parsed = safe_parse_item(item, i + 1)\n        if parsed:\n            successful_results.append(parsed)\n        else:\n            parse_errors += 1\n    \n    print(f\"‚úì Successfully parsed {len(successful_results)} items\")\n    if parse_errors > 0:\n        print(f\"‚ö† Failed to parse {parse_errors} items\")\n    \n    return successful_results\n\nif __name__ == '__main__':\n    # Test the robust search function\n    results = robust_craigslist_search('sfbay', 'sss', 'macbook')\n    \n    if results:\n        print(f\"\\nüìä Search Summary: {len(results)} valid results\")\n        print(\"\\n--- Top 5 Results ---\")\n        for i, result in enumerate(results[:5], 1):\n            print(f\"{i}. {result['title']}\")\n            print(f\"   üí∞ {result['price']}\")\n            print(f\"   üìç {result['location']}\")\n            if result['url']:\n                print(f\"   üîó {result['url']}\")\n            print(f\"   üìÑ Raw data length: {result['raw_item_length']} fields\")\n            print()\n    else:\n        print(\"‚ùå No valid results found\")"
          }
        }
      ]
    }
  ]
}